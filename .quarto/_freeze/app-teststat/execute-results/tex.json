{
  "hash": "951a1446a28f380166334bc7599da509",
  "result": {
    "engine": "knitr",
    "markdown": "# Mathematical Results for Standardized Statistics {#sec-app-teststat}\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nHere, we present some mathematical results associated with standardized statistics.  The material provided in this appendix is not necessary for understanding the concepts presented in the text.  It is provided only for those who are curious about the mathematics underlying some of the claims made throughout.\n\n## Single Mean Response, Same Signal\nIn @sec-teststat, we argued that the level of background noise can make it difficult to detect a signal.  That is illustrated in the following theorem.\n\n:::{#thm-app-teststat-same-signal}\nConsider two samples of the same size $n$.  Suppose the sample mean is equivalent in both samples, and consider testing the hypotheses\n\n$$H_0: \\mu = \\mu_0 \\qquad \\text{vs.} \\qquad H_1: \\mu \\neq \\mu_0$$\n\nwith each sample.  The difference in the sum of squares\n\n$$SS_0 - SS_1 = \\sum_{i=1}^{n} \\left(y_i - \\mu_0\\right)^2 - \\sum_{i=1}^{n} \\left(y_i - \\bar{y}\\right)^2$$\n\nwill be the same for each sample.\n:::\n\n:::{.proof}\nObserve that the difference in the sums of squares can be written as\n\n$$\n\\begin{aligned}\n  SS_0 - SS_1 \n    &= \\sum_{i=1}^{n} \\left(y_i - \\mu_0\\right)^2 - \\sum_{i=1}^{n} \\left(y_i - \\bar{y}\\right)^2 \\\\\n    &= \\sum_{i=1}^{n} \\left[\\left(y_i - \\mu_0\\right)^2 - \\left(y_i - \\bar{y}\\right)^2\\right] \\\\\n    &= \\sum_{i=1}^{n} \\left[y_i^2 - 2y_i \\mu_0 + \\mu_0^2 - y_i^2 + 2y_i \\bar{y} - \\bar{y}^2\\right] \\\\\n    &= \\sum_{i=1}^{n} \\left[\\mu_0^2 - 2y_i \\left(\\mu_0 - \\bar{y}\\right) - \\bar{y}^2\\right] \\\\\n    &= \\sum_{i=1}^{n} \\mu_0^2 - 2\\left(\\mu_0 - \\bar{y}\\right)\\sum_{i=1}^{n} y_i - \\sum_{i=1}^{n} \\bar{y}^2 \\\\\n    &= n\\mu_0^2 - 2n\\mu_0 \\bar{y} + 2n\\bar{y}^2 - n\\bar{y}^2 \\\\\n    &= n\\mu_0^2 - 2n\\mu_0 \\bar{y} + n\\bar{y}^2 \\\\\n    &= n \\left(\\mu_0 - \\bar{y}\\right)^2.\n\\end{aligned}\n$$\n\nThat is, the difference in the sum of squares is a function only of the null value and the sample mean.  Therefore, if two samples of the same size have the same sample mean, then the difference in the sums of squares will be equivalent for the two samples.\n:::\n\n\n## Single Mean Response, Two Standardized Statistics\nIn @sec-teststat, we presented the standardized statistic\n\n$$T_1^* = \\frac{SS_0 - SS_1}{s^2}.$$\n\nHowever, another popular standardized statistic for testing hypotheses for a single mean is \n\n$$T_2^* = \\frac{\\sqrt{n} \\left(\\bar{y} - \\mu_0\\right)}{s}.$$\n\nHowever, these two standardized statistics are related.\n\n:::{#thm-app-teststat-two-statistics}\nConsider a sample of size $n$ collected to test the following hypotheses:\n\n$$H_0: \\mu = \\mu_0 \\qquad \\text{vs.} \\qquad H_1: \\mu \\neq \\mu_0.$$\n\nDefine \n\n$$T_1^* = \\frac{SS_0 - SS_1}{s^2},$$\n\nwhere \n\n$$\n\\begin{aligned}\n  SS_0 &= \\sum_{i=1}^{n} \\left(y_i - \\mu_0\\right)^2 \\\\\n  SS_1 &= \\sum_{i=1}^{n} \\left(y_i - \\bar{y}\\right)^2.\n\\end{aligned}\n$$\n\nAnd, define\n\n$$T_2^* = \\frac{\\sqrt{n} \\left(\\bar{y} - \\mu_0\\right)}{s}$$\n\nwhere $s$ is the sample standard deviation.  Then, $T_1^* = \\left(T_2^*\\right)^2$.\n:::\n\n:::{.proof}\nFrom the proof of @thm-app-teststat-same-signal, we have that\n\n$$SS_0 - SS_1 = n \\left(\\mu_0 - \\bar{y}\\right)^2.$$\n\nWe can then rewrite $T_1^*$ as\n\n$$T_1^* = \\frac{n\\left(\\mu_0 - \\bar{y}\\right)^2}{s^2}.$$\n\nThis form of $T_1^*$ then makes the result clear.\n:::\n\n\n## Least Squares Estimate for Intercept-Only Model\nIn @sec-regquality, we state that the least squares estimate for the parameter $\\mu$ in a model of the data generating process that has the form\n\n$$(\\text{Response})_i = \\mu + \\varepsilon_i$$\n\nis the sample mean response.  While this is the intuitive estimate for $\\mu$ we presented in @sec-meanmodels, we had not yet related that to the method of least squares introduced in @sec-regmodel.  \n\n:::{#thm-app-teststat-ls-one}\nConsider a sample of size $n$; suppose we posit the following model for the data generating process:\n\n$$y_i = \\mu + \\varepsilon_i$$\n\nwhere $y_i$ represents the $i$-th observed value of the response variable and $\\mu$ is the sole parameter.  The least squares estimate of $\\mu$ is $\\bar{y}$, the sample mean.\n:::\n\n:::{.proof}\nObserve that, by definition (see @def-least-squares-estimates), the least squares estimate for this model is the value of $\\mu$ which minimizes the quantity\n\n$$\\sum_{i=1}^{n} \\left(y_i - \\mu\\right)^2.$$\n\nObserve that if we add and subtract the sample mean $\\bar{y}$ inside the squared quantity, we can rewrite the above quantity as\n\n$$\n\\begin{aligned}\n  \\sum_{i=1}^{n} \\left(y_i - \\mu\\right)^2 \n    &= \\sum_{i=1}^{n} \\left(y_i - \\bar{y} + \\bar{y} - \\mu\\right)^2 \\\\\n    &= \\sum_{i=1}^{n} \\left[\\left(y_i - \\bar{y}\\right)^2 + 2\\left(y_i - \\bar{y}\\right)\\left(\\bar{y} - \\mu\\right) + \\left(\\bar{y} - \\mu\\right)^2\\right] \\\\\n    &= \\sum_{i=1}^{n} \\left(y_i - \\bar{y}\\right)^2 + 2\\left(\\bar{y} - \\mu\\right) \\sum_{i=1}^{n}\\left(y_i - \\bar{y}\\right) + \\sum_{i=1}^{n}\\left(\\bar{y} - \\mu\\right)^2 \\\\\n    &= \\sum_{i=1}^{n} \\left(y_i - \\bar{y}\\right)^2 + 2\\left(\\bar{y} - \\mu\\right) \\left(n\\bar{y} - n\\bar{y}\\right) + n\\left(\\bar{y} - \\mu\\right)^2, \n\\end{aligned}\n$$\n\nwhere we make use of the fact that $\\bar{y} = \\frac{1}{n}\\sum_{i=1}^{n} y_i$ implies that $\\sum_{i=1}^{n} y_i = n\\bar{y}$.  Now, we notice that the second term in the above expansion resolves to 0.  Therefore, we have that\n\n$$\\sum_{i=1}^{n} \\left(y_i - \\mu\\right)^2 = \\sum_{i=1}^{n} \\left(y_i - \\bar{y}\\right)^2 + n\\left(\\bar{y} - \\mu\\right)^2.$$\n\nNotice that the second term is a squared difference; therefore, the second term must always be non-negative.  Therefore, the second term can only make the sum larger; further, the second term will reach 0, the smallest value possible, when $\\mu = \\bar{y}$.  Therefore, the sum is minimized when we take $\\mu = \\bar{y}$.  This gives that $\\bar{y}$ minimizes the quantity of interest.\n:::\n\nThis theorem allows us to conclude that the error sum of squares for a model of the form\n\n$$(\\text{Response})_i = \\mu + \\varepsilon_i$$\n\nis given by \n\n$$\\sum_{i=1}^{n} \\left[(\\text{Response})_i - (\\text{Overall Mean Response})\\right]^2,$$\n\nwhere the overall mean response is the sample mean response observed.\n\n\n## Class of Hypotheses that are Testable\n@sec-regquality developed a general standardized statistic for testing hypotheses in a linear model described by @eq-slr.  @sec-regextensions extended this standardized statistic to all linear models.  This standardized statistic allows us to have a unifying testing framework across the text.  However, not all hypotheses can be tested using this standardized statistic.  This framework applies to linear hypotheses.  For example, consider the model (@eq-slr):\n\n$$(\\text{Response})_i = \\beta_0 + \\beta_1 (\\text{Predictor})_i + \\varepsilon_i.$$\n\nFor this model, we can use the standardized statistic in @def-standard-f to test any null hypothesis of the form\n\n$$H_0: c_0\\beta_0 = c_2, \\ c_1\\beta_1 = c_3$$\n\nor \n\n$$H_0: c_0\\beta_0 + c_1\\beta_1 = c_2,$$\n\nwhere $c_0, c_1, c_2, c_3$ are known constants.  That is, we can test a hypothesis of the form\n\n$$H_0: \\beta_1 = 0 \\qquad \\text{vs.} \\qquad H_1: \\beta_1 \\neq 0$$\n\nbecause this can be written by setting $c_1 = 1$ and $c_0 = c_2 = c_3 = 0$ in the first form of the hypothesis given above.  We can also test a hypothesis of the form \n\n$$H_0: \\beta_1 = 0, \\beta_0 = 3 \\qquad \\text{vs.} \\qquad H_1: \\text{At least one } \\beta_j \\text{ differs}$$\n\nwhich corresponds to a reduce model of\n\n$$(\\text{Response})_i = 3 + \\varepsilon_i$$\n\nbecause this can be written by setting $c_0 = c_1 = 1$, $c_2 = 3$, and $c_3 = 0$ in the first form of the hypothesis given above.\n\nHowever, we cannot test a hypothesis of the form $\\beta_0\\beta_1 = 1$; while this may be a valid hypothesis, it cannot be written in either of the above forms described above.  We would need additional theory to test such a hypothesis.\n\n\n## Equivalence of Hypotheses with Blocking\nOur model for the data generating process comparing a quantitative response across the $k$ levels of a factor in the presence of $b$ blocks is\n\n$$(\\text{Response})_i = \\sum_{j=1}^{k} \\mu_j (\\text{Group } j)_i + \\sum_{m=2}^{b} \\beta_m (\\text{Block } m)_i + \\varepsilon_i$$\n\nwhere $(\\text{Group } j)$ and $(\\text{Block } m)$ are appropriately defined indicator variables.  Without loss of generality, consider the case where $k = 2$ and $b = 2$.  Then, our model reduces to\n\n$$(\\text{Response})_i = \\mu_1 (\\text{Group 1})_i + \\mu_2 (\\text{Group 2})_i + \\beta_2 (\\text{Block 2})_i + \\varepsilon_i.$$\n\nWe argue heuristically in @sec-blockmodel that testing the hypotheses\n\n$$H_0: \\mu_1 = \\mu_2 \\qquad \\text{vs.} \\qquad H_1: \\mu_1 \\neq \\mu_2$$\n\nwhich are hypotheses about the average response _in Block 1_ is equivalent to testing hypotheses about the overall average response across all blocks.  To establish this more formally, note that based on the above model for the data generating process, the overall average response for Group 1, call it $\\theta_1$, is given by\n\n$$\\theta_1 = \\frac{1}{2} \\mu_1 + \\frac{1}{2} \\left(\\mu_1 + \\beta_2\\right) = \\mu_1 + \\frac{\\beta_2}{2},$$\n\nwhere we average the average response from Block 1 with that of Block 2.  Similarly, the average response for Group 2, call it $\\theta_2$, is given by\n\n$$\\theta_2 = \\frac{1}{2} \\mu_2 + \\frac{1}{2} \\left(\\mu_2 + \\beta_2\\right) = \\mu_2 + \\frac{\\beta_2}{2}.$$\n\nNow, note that $\\theta_1 - \\theta_2 = \\mu_1 - \\mu_2$; therefore, $\\theta_1 = \\theta_2$ if and only if $\\mu_1 = \\mu_2$.  That is, testing whether the overall average response across all individuals is equal across groups is equivalent to testing whether the average response within Block 1 is equal across groups.",
    "supporting": [
      "app-teststat_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {
      "knitr": [
        "{\"type\":\"list\",\"attributes\":{},\"value\":[]}"
      ]
    },
    "preserve": null,
    "postProcess": false
  }
}