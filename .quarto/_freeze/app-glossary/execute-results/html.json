{
  "hash": "80e58380a29a2a518a22e3128a164010",
  "result": {
    "engine": "knitr",
    "markdown": "# Glossary\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n## Five Fundamental Ideas of Inference\nThis text revolves around five fundamental ideas of inference.  These were introduced in the text, and they are provided here for quick reference along with a link to where they were first introduced.\n\n:::{.callout-important}\n## Fundamental Idea I (@sec-questions)\nA research question can often be framed in terms of a parameter that characterizes the population.  Framing the question should then guide our analysis.\n:::\n\n:::{.callout-important}\n## Fundamental Idea II (@sec-data)\nIf data is to be useful for making conclusions about the population, a process referred to as drawing inference, proper data collection is crucial.  Randomization can play an important role ensuring a sample is representative and that inferential conclusions are appropriate.\n:::\n\n:::{.callout-important}\n## Fundamental Idea III (@sec-summaries)\nThe use of data for decision making requires that the data be summarized and presented in ways that address the question of interest and represent the variability present.\n:::\n\n:::{.callout-important}\n## Fundamental Idea IV (@sec-samplingdistns)\nVariability is inherent in any process, and as a result, our estimates are subject to sampling variability.  However, these estimates often vary across samples in a predictable way; that is, they have a distribution that can be modeled.\n:::\n\n:::{.callout-important}\n## Fundamental Idea V (@sec-nulldistns)\nWith a model for the distribution of a statistic under a proposed model, we can quantify the the likelihood of an observed sample under that proposed model.  This allows us to draw conclusions about the corresponding parameter, and therefore the population, of interest.\n:::\n\n\n## Distributional Quartet\nThis text refers to what we call the \"distributional quartet\" --- the four key distributions that are central to nearly any analysis.  These are introduced early in the text and are \n\n  - The distribution of the population; this characterizes the pattern of variability of a variable across individual units in the population.  While this is not directly observed, we sometimes posit a model for this distribution.\n  - The distribution of the sample; this characterizes the pattern of variability of a variable across individual units in the sample.  This is what we summarize (graphically/numerically) using the available data.\n  - The sampling distribution of the statistic; this characterizes the pattern of variability of a statistic across repeated samples.  While this is not directly observed, we model it by applying conditions on the stochastic portion of the model for the data generating process.\n  - The null distribution of a (often standardized) statistic; this is the sampling distribution of a statistic when a specified null hypothesis is enforced. \n\n\n\n## Models for the Data Generating Process\nThis text takes a modeling approach to inference.  The following models are introduced in the text; each model is presented with a link to where the model was fully defined in the text.\n\n:::{.callout-important}\n## Data Generating Process for Single Mean Response (@eq-single-mean)\nIn general, given a quantitative response variable and no predictors, our model for the data generating process is\n\n$$(\\text{Response})_i = \\mu + \\epsilon_i$$\n  \nwhere $\\mu$ represents the average response in the population, the parameter of interest.\n:::\n\n:::{.callout-important}\n## Simple Linear Regression Model (@eq-slr)\nFor a quantitative response and a quantitative predictor, the general form of the simple linear regression model is\n\n$$(\\text{Response})_i = \\beta_0 + \\beta_1 (\\text{Predictor})_i + \\varepsilon_i$$\n\nwhere $\\beta_0$ and $\\beta_1$ are parameters governing the model for the data generating process.\n:::\n\n\n:::{.callout-important}\n## General Linear Regression Model (@eq-mlr)\nFor a quantitative response and one or more predictors, the general form of the linear regression model is\n\n$$\n\\begin{aligned}\n  (\\text{Response})_i \n    &= \\beta_0 + \\beta_1 (\\text{Predictor 1})_i + \\beta_2(\\text{Predictor 2})_i + \\dotsb + \\beta_p (\\text{Predictor } p)_i + \\varepsilon_i \\\\\n    &= \\beta_0 + \\sum_{j=1}^{p} \\beta_j (\\text{Predictor j})_i + \\varepsilon_i\n\\end{aligned}\n$$\n\nwhere $\\beta_j$ for $j = 0, 1, 2, \\dotsc, p$ are the $p + 1$ parameters governing the model for the data generating process.\n:::\n\n\n:::{.callout-important}\n## ANOVA Model (@eq-anova)\nFor a quantitative response and a single categorical predictor (also known as a factor) with $k$ levels, the ANOVA model is\n\n$$(\\text{Response})_i = \\sum_{j = 1}^{k} \\mu_j (\\text{Group } j)_i + \\varepsilon_i$$\n\nwhere\n\n$$(\\text{Group } j)_i = \\begin{cases} 1 & \\text{i-th unit belongs to group } j \\\\ 0 & \\text{otherwise} \\end{cases}$$\n\nis an indicator variable capturing whether a unit belongs to the $j$-th group and $\\mu_1, \\mu_2, \\dotsc, \\mu_k$ are the parameters governing the model for the data generating process.\n:::\n\n\n:::{.callout-important}\n## Repeated Measures ANOVA Model (@eq-blocks)\nFor a quantitative response and a single categorical predictor (also known as a factor) with $k$ levels in the presence of $b$ blocks, the repeated measures ANOVA model is\n\n$$(\\text{Response})_i = \\sum_{j = 1}^{k} \\mu_j (\\text{Group } j)_i + \\sum_{m = 2}^{b} \\beta_m (\\text{Block } m)_i + \\varepsilon_i$$ \n\nwhere\n\n$$\n\\begin{aligned}\n  (\\text{Group } j)_i \n    &= \\begin{cases} 1 & \\text{i-th unit belongs to group } j \\\\ 0 & \\text{otherwise} \\end{cases} \\\\\n  (\\text{Block } m)_i \n    &= \\begin{cases} 1 & \\text{i-th unit belongs to block } m \\\\ 0 & \\text{otherwise} \\end{cases}\n\\end{aligned}\n$$ \n\nare indicator variables capturing whether a unit belongs to the $j$-th group and $m$-th block, respectively; and, $\\mu_1, \\mu_2, \\dotsc, \\mu_k$ and $\\beta_2, \\beta_3, \\dotsc, \\beta_b$ are the parameters governing the model for the data generating process.\n\nThis model assumes any differences between groups are similar across all blocks.\n:::\n\n\n## Glossary\nThe following key terms were defined in the text; each term is presented with a link to where the term was first encountered in the text.\n\n\nAlternative Hypothesis (@def-alternative-hypothesis) \n: The statement (or theory) about the parameter capturing what we would like to provide evidence _for_; this is the opposite of the null hypothesis.  This is denoted $H_1$ or $H_a$, read \"H-one\" and \"H-A\" respectively.\n\nAverage (@def-average) \n: Also known as the \"mean,\" this measure of location represents the balance point for the distribution.  If $x_i$ represents the $i$-th value of the variable $x$ in the sample, the sample mean is typically denoted by $\\bar{x}$.  \n\nFor a sample of size $n$, it is computed by\n$$\\bar{x} = \\frac{1}{n}\\sum_{i=1}^{n} x_i.$$\n\nWhen referencing the average for a population, the mean is also called the \"Expected Value,\" and is often denoted by $\\mu$.\n\nBetween Group Variability (@def-between-group-variability) \n: When comparing a quantitative response across groups, the between group variability is the variability in the average response from one group to another.\n\nBias (@def-bias) \n: A set of measurements is said to be biased if they are _consistently_ too high (or too low).  Similarly, an estimate of a parameter is said to be biased if it is _consistently_ too high (or too low).\n\nBlocking (@def-blocking) \n: Blocking is a way of minimizing the variability contributed by an inherent characteristic that results in dependent observations.  In some cases, the blocks are the unit of observation which is sampled from a larger population, and multiple observations are taken on each unit.  In other cases, the blocks are formed by grouping the units of observations according to an inherent characteristic; in these cases that shared characteristic can be thought of having a value that was sampled from a larger population.\n\nIn both cases, the observed blocks can be thought of as a random sample; within each block, we have multiple observations, and the observations from the same block are more similar than observations from different blocks.\n\nBootstrapping (@def-bootstrap) \n: A method of modeling the sampling distribution by repeatedly resampling from the original data.\n\nCategorical Variable (@def-categorical) \n: Also called a \"qualitative variable,\" a measurement on a subject which denotes a grouping or categorization.\n\nClassical ANOVA Model (@def-classical-anova) \n: For a quantitative response and single categorical predictor with $k$ levels, the classical ANOVA model assumes the following data generating process:\n\n$$(\\text{Response})_i = \\sum_{j=1}^{k} \\mu_j (\\text{Group } j)_i + \\varepsilon_i$$\n\nwhere\n\n$$\n(\\text{Group } j)_{i} = \\begin{cases}\n  1 & \\text{if i-th observation belongs to group } j \\\\\n  0 & \\text{otherwise}\n  \\end{cases}\n$$\n\nare indicator variables and where\n\n  1. The error in the response for one subject is independent of the error in the response for all other subjects.\n  2. The variability in the error of the response within each group is the same across all groups.\n  3. The errors follow a Normal Distribution.\n  \nThis is the default \"ANOVA\" analysis implemented in the majority of statistical packages.\n\nClassical Regression Model (@def-classical-regression) \n: For a quantitative response and single predictor, the classical regression model assumes the following data generating process:\n\n$$(\\text{Response})_i = \\beta_0 + \\beta_1 (\\text{Predictor})_{i} + \\epsilon_i$$\n\nwhere \n\n  1. The error in the response has a mean of 0 for all values of the predictor.\n  2. The error in the response for one subject is independent of the error in the response for all other subjects.\n  3. The variability in the error of the response is the same for all values of the predictor.\n  4. The errors follow a Normal Distribution.\n  \nThis is the default \"regression\" analysis implemented in the majority of statistical packages.\n\nClassical Repeated Measures ANOVA Model (@def-classical-repeated-measures-anova) \n: For a quantitative response and single categorical predictor with $k$ levels in the presence of $b$ blocks, the classical repeated measures ANOVA model assumes the following data generating process:\n\n$$(\\text{Response})_i = \\sum_{j=1}^{k} \\mu_j (\\text{Group } j)_i + \\sum_{m=2}^{b} \\beta_m (\\text{Block } m)_i + \\varepsilon_i$$\n\nwhere\n\n$$\n\\begin{aligned}\n  (\\text{Group } j)_{i} &= \\begin{cases}\n    1 & \\text{if i-th observation belongs to group j} \\\\\n    0 & \\text{otherwise}\n    \\end{cases} \\\\\n  (\\text{Block } m)_{i} &= \\begin{cases}\n    1 & \\text{if i-th observation belongs to block m} \\\\\n    0 & \\text{otherwise}\n    \\end{cases}\n\\end{aligned}\n$$\n\nare indicator variables and where\n\n  1. The error in the response for one subject is independent of the error in the response for all other subjects.\n  2. The variability in the error of the response is the same across all predictors.\n  3. The errors follow a Normal distribution.\n  4. Any differences between the groups are similar across all blocks.  This results from the deterministic portion of the model for the data generating process being correctly specified and is equivalent to saying the error in the response, on average, takes a value of 0 for all predictors.\n  5. The effect of a block on the response is independent of the effect of any other block on the response.\n  6. The effect of a block on the response is independent of the error in the response for all subjects.\n  7. The block effects follow a Normal distribution.\n\nThis is the default \"repeated measures ANOVA\" analysis implemented in the majority of statistical packages.\n\nCodebook (@def-codebook) \n: Also called a \"data dictionary,\" these provide complete information regarding the variables contained within a dataset.\n\nConfidence Interval (@def-confidence-interval) \n: An interval (range of values) estimate of a parameter that incorporates the variability in the statistic.  The process of constructing a $k$% confidence interval results in these intervals containing the parameter of interest in $k$% of repeated studies.  The value of $k$ is called the _confidence level_.\n\nConfounding (@def-confounding) \n: When the effect of a variable on the response is mis-represented due to the presence of a third, potentially unobserved, variable known as a confounder.\n\nControlled Experiment (@def-controlled-experiment) \n: A study in which each subject is _randomly_ assigned to one of the groups being compared in the study.\n\nCorrelation Coefficient (@def-correlation-coefficient) \n: A numerical measure of the _strength_ and _direction_ of the _linear_ relationship between two quantitative variables.\n\nThe classical Pearson Correlation Coefficient $r$ is given by the following formula:\n\n$$r = \\frac{\\sum_{i=1}^{n} \\left(x_i - \\bar{x}\\right)\\left(y_i - \\bar{y}\\right)}{\\sqrt{\\sum_{i=1}^n \\left(x_i - \\bar{x}\\right)^2 \\sum_{i=1}^n \\left(y_i - \\bar{y}\\right)^2}}$$\n\nwhere $\\bar{x}$ and $\\bar{y}$ represent the sample means of the predictor and response, respectively.\n\nDegrees of Freedom (@def-df) \n: A measure of the flexibility in a sum of squares term; when a sum of squares is divided by the corresponding degrees of freedom, the result is a variance term.\n\nDeterministic Process (@def-deterministic-process) \n: A process for which the output is completely determined by the input(s).  That is, the output can be determined with certainty.\n\nDistribution (@def-distribution) \n: The pattern of variability corresponding to a set of values.\n\nDistribution of the Population (@def-distribution-population) \n: The pattern of variability in values of a variable at the population level.  Generally, this is impossible to know, but we might model it.\n\nDistribution of the Sample (@def-distribution-sample) \n: The pattern of variability in the observed values of a variable.\n\nError Sum of Squares (@def-sse) \n: The Error Sum of Squares, abbreviated SSE and sometimes referred to as the Residual Sum of Squares, is given by\n\n$$SSE = \\sum_{i=1}^{n} \\left[(\\text{Response})_i - (\\text{Predicted Mean Response})_i\\right]^2$$\n\nwhere the predicted mean response is computed using the least squares estimates.\n\nEstimation (@def-estimation) \n: Using the sample to approximate the value of a parameter from the underlying population.\n\nExtrapolation (@def-extrapolation) \n: Using a model to predict outside of the region for which data is available.\n\nFactor (@def-factor) \n: Also referred to as the \"treatment\" in some settings, a factor is a categorical predictor.  The categories represented by this categorical variable are called \"levels.\"\n\nFrequency (@def-frequency) \n: The number of observations in a sample falling into a particular group (level) defined by a categorical variable.\n\nHypothesis Testing (@def-hypothesis-testing) \n: Using a sample to determine if the data is consistent with a working theory or if there is evidence to suggest the data is not consistent with the theory.\n\nIdentically Distributed (@def-identically-distributed) \n: A set of random variables is said to be identically distributed if they are from the same population.\n\nSimilarly, a set of observations is said to be identically distributed if they share the same data generating process.\n\nIndependence (@def-independence) \n: Two random variables are said to be independent when the likelihood that one random variable takes on a particular value does not depend on the value of the other random variable.  \n\nSimilarly, two observations are said to be independent when the likelihood that one observation takes on a particular value does not depend on the value of the other observation.\n\nIndicator Variable (@def-indicator-variable) \n: An indicator variable is a binary (takes the value 0 or 1) variable used to represent whether an observation belongs to a specific group defined by a categorical variable.\n\nInteraction Term (@def-interaction-term) \n: A variable resulting from taking the product of two predictors in a regression model.  The product allows the effect of one predictor to depend on another predictor, essentially modifying the effect.\n\nInterquartile Range (@def-interquartile-range) \n: Often abbreviated as IQR, this is the distance between the first and third quartiles.  This measure of spread indicates the range over which the middle 50% of the data is spread.\n\nLaw of Large Numbers (@def-lln) \n: For our purposes, the Law of Large Numbers essentially says that as a sample size gets infinitely large, a statistic will become arbitrarily close (extremely good approximation) of the parameter it estimates.\n\nLeast Squares Estimates (@def-least-squares-estimates) \n: Often called the \"best fit line,\" these are the estimates of the parameters in a regression model chosen to minimize the sum of squared errors.  Formally, for @eq-slr, they are the values of $\\beta_0$ and $\\beta_1$ which minimize the quantity \n\n$$\\sum_{i=1}^n \\left[(\\text{Response})_i - \\beta_0 - \\beta_1(\\text{Predictor})_{i}\\right]^2.$$\n\nThe resulting estimates are often denoted by $\\widehat{\\beta}_0$ and $\\widehat{\\beta}_1$.\n\nLeast Squares Estimates for General Linear Model (@def-mlr-least-squares-estimates) \n: The least squares estimates for a general linear model (@eq-mlr) are the values of $\\beta_0, \\beta_1, \\beta_2, \\dotsc, \\beta_p$ which minimize the quantity\n\n$$\\sum_{i=1}^n \\left[(\\text{Response})_i - \\beta_0 - \\sum_{j=1}^{p} \\beta_j(\\text{Predictor } j)_{i}\\right]^2.$$\n\nMean Square (@def-ms) \n: A mean square is the ratio of a sum of squares and its corresponding degrees of freedom.  For a model of the form in @eq-slr, we have\n\n  - __Mean Square Total (MST)__: estimated variance of the responses; this is the same as the sample variance of the response.\n  - __Mean Square for Regression (MSR)__: estimated variance of the predicted responses.\n  - __Mean Square Error (MSE)__: estimated variance of the error terms; this is equivalent to the estimated variance of the response for a given value of the predictor (the variance of the response about the regression line).\n  \nIn each case, the mean square is an estimated variance.\n\nMean Square (in ANOVA) (@def-ms-anova) \n: A mean square is the ratio of a sum of squares and its corresponding degrees of freedom.  For a model of the form in @eq-anova, we have\n\n  - __Mean Square Total (MST)__: estimated variance of the responses; this is the same as the sample variance of the response.\n  - __Mean Square for Regression (MSR)__: estimated variance of the sample mean responses from each group; this is also called the Mean Square for Treatment (MSTrt) in ANOVA.\n  - __Mean Square Error (MSE)__: estimated variance of the error terms; this is equivalent to the estimated variance of the response within a group.\n  \nIn each case, the mean square is an estimated variance.  These are equivalent to the MST, MSR, and MSE in the regression model (@def-ms).\n\nMultivariable (@def-multivariable) \n: This term refers to questions of interest which involve more than a single variable.  Often, these questions involve many variables.  Multivariable models typically refer to a model with two or more predictors.\n\nNormal Distribution (@def-normal-distribution) \n: Also called the Gaussian Distribution, this probability model is popular for modeling noise within a data generating process.  It has the following characteristics:\n\n  - It is bell-shaped.\n  - It is symmetric, meaning the mean is directly at its center, and the lower half of the distribution looks like a mirror image of the upper half of the distribution.\n  - Often useful for modeling noise due to natural phenomena or sums of measurements.\n  \nThe functional form of the Normal distribution is\n\n$$f(x) = \\frac{1}{\\sqrt{2\\pi\\sigma^2}} e^{-\\frac{1}{2\\sigma^2}(x - \\mu)^2}$$\n\nwhere $\\mu$ is the mean of the distribution and $\\sigma^2$ is the variance of the distribution.\n\nNull Distribution (@def-null-distribution) \n: The sampling distribution of a statistic _when_ the null hypothesis is true.\n\nNull Hypothesis (@def-null-hypothesis) \n: The statement (or theory) about the parameter that we would like to _disprove_.  This is denoted $H_0$, read \"H-naught\" or \"H-zero\".\n\nNull Value (@def-null-value) \n: The value associated with the equality component of the null hypothesis; it forms the threshold or boundary between the hypotheses.  Note: not all questions of interest require a null value be specified.\n\nNumeric Variable (@def-numeric) \n: Also called a \"quantitative variable,\" a measurement on a subject which takes on a numeric value _and_ for which ordinary arithmetic makes sense.\n\nObservational Study (@def-observational-study) \n: A study in which each subject \"self-selects\" into one of groups being compared in the study.  The phrase \"self-selects\" is used very loosely here and can include studies for which the groups are defined by an inherent characteristic or are chosen haphazardly.\n\nOutlier (@def-outlier) \n: An individual observation which is so extreme, relative to the rest of the observations in the sample, that it does not appear to conform to the same distribution.\n\nP-Value (@def-pvalue) \n: The probability, assuming the null hypothesis is true, that we would observe a statistic, from sampling variability alone, as extreme or more so as that observed in our sample.  The p-value quantifies the strength of evidence against the null hypothesis, with smaller values indicating stronger evidence.\n\nParameter (@def-parameter) \n: Numeric quantity which summarizes the distribution of a variable within the _population_ of interest.  Generally denoted by Greek letters in statistical formulas.\n\nPercentile (@def-percentile) \n: The $k$-th percentile is the value $q$ such that $k$% of the values in the distribution are less than or equal to $q$.  For example,\n\n  - 25% of values in a distribution are less than or equal to the 25-th percentile (known as the \"first quartile\" and denoted $Q_1$).\n  - 50% of values in a distribution are less than or equal to the 50-th percentile (known as the \"median\").\n  - 75% of values in a distribution are less than or equal to the 75-th percentile (known as the \"third quartile\" and denoted $Q_3$).\n\nPopulation (@def-population) \n: The collection of subjects we would like to say something about.\n\nPower (@def-power) \n: In statistics, power refers to the probability that a study will discern a signal when one really exists in the data generating process.  More technically, it is the probability a study will provide evidence against the null hypothesis when the null hypothesis is false.\n\nProbability Plot (@def-probability-plot) \n: Also called a \"Quantile-Quantile Plot\", a probability plot is a graphic for comparing the distribution of an observed sample with a theoretical probability model for the distribution of the underlying population.  The quantiles observed in the sample are plotted against those expected under the theoretical model.\n\nR-Squared (@def-r-squared) \n: Sometimes reported as a percentage, the R-Squared value measures the proportion of the variability in the response explained by a model.  It is given by \n\n$$\\text{R-squared} = \\frac{SSR}{SST}.$$\n\nRandomization (@def-randomization) \n: Randomization can refer to random _selection_ or random _allocation_.  Random selection refers to the use of a random mechanism (e.g., a simple random sample, @def-simple-random-sample, or a stratified random sample, @def-stratified-random-sample) to select units from the population.  Random selection minimizes bias.\n\nRandom allocation refers to the use of a random mechanism when assigning units to a specific treatment group in a controlled experiment (@def-controlled-experiment).  Random allocation eliminates confounding and permits causal interpretations.\n\nRandomized Complete Block Design (@def-rcbd) \n: A randomized complete block design is an example of a controlled experiment utilizing blocking.  Each treatment is randomized to observations within blocks such that within each block every treatment is present and the same number of observations are assigned to each treatment.\n\nReduction of Noise (@def-noise-reduction) \n: Reducing extraneous sources of variability can be accomplished by fixing extraneous variables or blocking (@def-blocking).  These actions reduce the number of differences between the units under study.\n\nReference Group (@def-reference-group) \n: The group defined by setting all indicator variables in a model for the data generating process equal to 0.\n\nRegression (@def-regression) \n: Used broadly, this refers to the process of fitting a statistical model for the data generating process to observed data.  More specifically, it is a process of estimating the parameters in a data generating process using observed data.\n\nRegression Sum of Squares (@def-ssr) \n: The Regression Sum of Squares, abbreviated SSR, is given by\n\n$$SSR = \\sum_{i=1}^{n} \\left[(\\text{Predicted Mean Response})_i - (\\text{Overall Mean Response})\\right]^2$$\n\nwhere the predicted mean response is computed using the least squares estimates and the overall mean response is the sample mean.\n\nRelative Frequency (@def-relative-frequency) \n: Also called the \"proportion,\" the fraction of observations falling into a particular group (level) of a categorical variable.\n\nReplication (@def-replication) \n: Replication results from taking measurements on different units (or subjects), for which you expect the results to be similar.  That is, any variability across the units is due to natural variability within the population.\n\nResidual (@def-residual) \n: The difference between the observed response and the predicted response (estimated deterministic portion of the model). Specifically, the residual for the $i$-th observation is given by\n\n$$(\\text{Residual})_i = (\\text{Response})_i - (\\text{Predicted Mean Response})_i$$\n\nwhere the \"predicted mean response\" is often called the predicted, or fitted, value.\n\nResiduals mimic the noise in the data generating process.\n\nResponse (@def-response) \n: The primary variable of interest within a study.  This is the variable you would either like to explain or estimate.\n\nSample (@def-sample) \n: The collection of subjects for which we actually obtain measurements (data).\n\nSampling Distribution (@def-sampling-distribution) \n: The distribution of a _statistic_ across repeated samples (of the same size) from the population.\n\nSimple Random Sample (@def-simple-random-sample) \n: Often abbreviated SRS, this is a sample of size $n$ such that _every_ collection of size $n$ is equally likely to be the resulting sample.  This is equivalent to a lottery.\n\nStandard Deviation (@def-standard-deviation) \n: A measure of spread, this is the square root of the variance.\n\nStandard Error (@def-standard-error) \n: The standard error is the estimated standard deviation of a _statistic_; that is, it is the standard deviation from a _model_ for the sampling distribution of a statistic. It quantifies the variability in the statistic across repeated samples.\n\nStandardized (Test) Statistic (@def-standardized-test-statistic) \n: Also, known as a test statistic, a standardized statistic is a ratio of the signal in the sample to the noise in the sample.  The larger the standardized statistic, the stronger the evidence of a signal; said another way, the larger the standardized statistic, the stronger the evidence against the null hypothesis.\n\nStandardized Statistic for ANOVA (@def-anova-f) \n: Consider testing a set of hypotheses for a model of the data generating process of the form (@eq-anova):\n\n$$(\\text{Response})_i = \\sum_{j=1}^{k} \\mu_j(\\text{Group } j)_i + \\varepsilon_i,$$\n\nwhere \n\n$$(\\text{Group } j)_i = \\begin{cases} 1 & \\text{i-th unit belongs to group } j \\\\ 0 & \\text{otherwise} \\end{cases}$$ \n\nis an indicator variable.  Denote this model as Model 1, and denote the model that results from applying the parameter constraints defined under the null hypothesis as Model 0.  A standardized statistic, sometimes called the \"standardized F statistic,\" for testing the hypotheses is given by\n\n$$T^* = \\frac{\\left(SSE_0 - SSE_1\\right) / (k - r)}{SSE_1 / (n - k)},$$\n\nwhere $k$ is the number of parameters in the full unconstrained model and $r$ is the number of parameters in the reduced model.  Defining \n\n$$MSA = \\frac{SSE_0 - SSE_1}{k - r}$$\n\nto be the \"mean square for additional terms,\" which captures the shift in the error sum of squares from the reduced model to the full unconstrained model, we can write the standardized statistic as\n\n$$T^* = \\frac{MSA}{MSE}$$\n\nwhere the mean square error in the denominator comes from the full unconstrained model.  Just as before, the MSE represents the residual variance --- the variance in the response for a particular set of the predictors.\n\nStandardized Statistic for General Linear Model (@def-general-f) \n: Consider testing a set of hypotheses for a model of the data generating process of the form (@eq-mlr):\n\n$$(\\text{Response})_i = \\beta_0 + \\sum_{j=1}^{p} \\beta_j(\\text{Predictor } j)_i + \\varepsilon_i.$$\n\nDenote this model as Model 1, and denote the model that results from applying the parameter constraints defined under the null hypothesis as Model 0.  A standardized statistic, sometimes called the \"nested F statistic,\" for testing the hypotheses is given by\n\n$$T^* = \\frac{\\left(SSE_0 - SSE_1\\right) / (p + 1 - r)}{SSE_1 / (n - p - 1)},$$\n\nwhere $p + 1$ is the number of parameters in the full unconstrained model (including the intercept) and $r$ is the number of parameters in the reduced model.  Defining \n\n$$MSA = \\frac{SSE_0 - SSE_1}{p + 1 - r}$$\n\nto be the \"mean square for additional terms,\" which captures the shift in the error sum of squares from the reduced model to the full unconstrained model, we can write the standardized statistic as\n\n$$T^* = \\frac{MSA}{MSE}$$\n\nwhere the mean square error in the denominator comes from the full unconstrained model.  Just as before, the MSE represents the residual variance --- the variance in the response for a particular set of the predictors.\n\nStandardized Statistic for Repeated Measures ANOVA (@def-blocks-f) \n: Consider testing a set of hypotheses for a model of the data generating process of the form (@eq-blocks):\n\n$$(\\text{Response})_i = \\sum_{j=1}^{k} \\mu_j (\\text{Group } j)_i + \\sum_{m=2}^{b} \\beta_m (\\text{Block } m)_i + \\varepsilon_i$$\n\nwhere\n\n$$\n\\begin{aligned}\n  (\\text{Group } j)_i \n    &= \\begin{cases} 1 & \\text{if i-th observation corresponds to group } j \\\\ 0 & \\text{otherwise} \\end{cases} \\\\\n  (\\text{Block } m)_i\n    &= \\begin{cases} 1 & \\text{if i-th observation corresponds to block } m \\\\ 0 & \\text{otherwise} \\end{cases}\n\\end{aligned}\n$$\n\nare indicator variables.  Denote this model as Model 1, and denote the model that results from applying the parameter constraints defined under the null hypothesis as Model 0.  A standardized statistic, sometimes called the \"standardized F statistic,\" for testing the hypotheses is given by\n\n$$T^* = \\frac{\\left(SSE_0 - SSE_1\\right) / (k + b - 1 - r)}{SSE_1 / (n - k - b + 1)},$$\n\nwhere $k + b - 1$ is the number of parameters in the full unconstrained model and $r$ is the number of parameters in the reduced model.  Defining\n\n$$MSA = \\frac{SSE_0 - SSE_1}{k + b - 1 - r}$$\n\nto be the \"mean square for additional terms,\" which captures the shift in the error sum of squares from the reduced model to the full unconstrained model, we can write the standardized statistic as\n\n$$T^* = \\frac{MSA}{MSE}$$\n\nwhere the mean square error in the denominator comes from the full unconstrained model.  Just as before, the MSE represents the residual variance --- the variance in the response for a particular set of predictors.\n\nStandardized Statistic for Simple Linear Regression (@def-standard-f) \n: Consider testing a set of hypotheses for a model of the data generating process of the form (@eq-slr):\n\n$$(\\text{Response})_i = \\beta_0 + \\beta_1(\\text{Predictor})_i + \\varepsilon_i.$$\n\nDenote this model as Model 1, and denote the model that results from applying the parameter constraints defined under the null hypothesis as Model 0[^Fcaveat].  A standardized statistic, sometimes called the \"standardized F statistic,\" for testing the hypotheses is given by\n\n$$T^* = \\frac{\\left(SSE_0 - SSE_1\\right) / (2 - r)}{SSE_1 / (n - 2)},$$\n\nwhere $r$ is the number of parameters in the reduced model.  Defining \n\n$$MSA = \\frac{SSE_0 - SSE_1}{2 - r}$$\n\nto be the \"mean square for additional terms,\" which captures the shift in the error sum of squares from the reduced model to the full unconstrained model, we can write the standardized statistic as\n\n$$T^* = \\frac{MSA}{MSE}$$\n\nwhere the mean square error in the denominator comes from the full unconstrained model.\n\nStatistic (@def-statistic) \n: Numeric quantity which summarizes the distribution of a variable within a _sample_.\n\nStatistical Inference (@def-inference) \n: The process of using a sample to characterize some aspect of the underlying population.\n\nStochastic Process (@def-stochastic-process) \n: A process for which the output cannot be predicted with certainty.\n\nStratified Random Sample (@def-stratified-random-sample) \n: A sample in which the population is first divided into groups, or strata, based on a characteristic of interest; a simple random sample is then taken within each group.\n\nTime-Series Plot (@def-time-series-plot) \n: A time-series plot of a variable is a line plot with the variable on the y-axis and time on the x-axis.\n\nTotal Sum of Squares (@def-sst) \n: The Total Sum of Squares, abbreviated SST, is given by\n\n$$SST = \\sum_{i=1}^{n} \\left[(\\text{Response})_i - (\\text{Overall Mean Response})\\right]^2$$\n\nwhere the overall average response is the sample mean.\n\nVariability (@def-variability) \n: The notion that measurements differ from one observation to another.\n\nVariable (@def-variable) \n: A measurement, or category, describing some aspect of the subject.\n\nVariance (@def-variance) \n: A measure of spread, this roughly captures the average distance values in the distribution are from the mean.\n\nFor a sample of size $n$, it is computed by\n$$s^2 = \\frac{1}{n-1}\\sum_{i=1}^{n} \\left(x_i - \\bar{x}\\right)^2$$\n\nwhere $\\bar{x}$ is the sample mean and $x_i$ is the $i$-th value in the sample.  The division by $n-1$ instead of $n$ removes bias in the statistic.\n\nThe symbol $\\sigma^2$ is often used to denote the variance in the population.\n\nWithin Group Variability (@def-within-group-variability) \n: When comparing a quantitative response across groups, the within group variability is the variability in the response within each group.\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}