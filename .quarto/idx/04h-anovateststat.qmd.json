{"title":"Quantifying the Evidence","markdown":{"headingText":"Quantifying the Evidence","headingAttr":{"id":"sec-anovateststat","classes":[],"keyvalue":[]},"containsRefs":false,"markdown":"\n{{< include _setupcode.qmd >}}\n\nIn the previous two chapters, we described a model for describing the data generating process for a quantitative response as a function of a single categorical predictor:\n\n$$(\\text{Response})_i = \\sum_{j = 1}^{k} \\mu_j (\\text{Group } j)_i + \\varepsilon_i$$\n\nwhere\n\n$$(\\text{Group } j)_i = \\begin{cases} 1 & \\text{i-th unit belongs to group } j \\\\ 0 & \\text{otherwise} \\end{cases}$$ \n\nis an indicator variable.\n\n@sec-anovamodel discussed obtaining estimates of these unknown parameters using the method of least squares.  @sec-anovaconditions imposed conditions on the stochastic portion of the model in order to develop a confidence interval for each parameter.  In this chapter, we turn to performing inference through the computation of a p-value for a set of hypotheses.  As we saw with regression models in @sec-regquality, this is accomplished through partitioning variability.  \n\n@fig-anovateststat-boxplots displays a numeric response across three groups for two different datasets.  Consider the following question:\n\n  > For which dataset is there _stronger_ evidence that the response is associated with the grouping variable?\n\n```{r}\n#| label: fig-anovateststat-boxplots\n#| fig-cap: Simulated data illustrating that signal strength is determined by partitioning variability. There is a clear signal (difference in the location across groups) for Dataset A but not for Dataset B.\n#| fig-alt: Two datasets are summarized by side-by-side boxplots for three groups.  The boxplots in Dataset A are centered in the same locations as in Dataset B; however, the spread within each boxplot is much larger for Dataset B.\nset.seed(20170828)\n\ndat <- tibble(\n  dataset = rep(c(\"Dataset A\", \"Dataset B\"), each = 90),\n  group = rep(c(\"Group I\", \"Group II\", \"Group III\"), each = 30, times = 2),\n  mids = rep(c(5, 6, 7), each = 30, times = 2),\n  halfrange = rep(c(1, 5), each = 90),\n  y = runif(180, min = mids - halfrange, max = mids + halfrange)\n)\n\ndat <- dat |>\n  group_by(dataset, group) |>\n  mutate(y = scale(y, center=TRUE, scale=FALSE),\n         y = y + mids) |>\n  ungroup()\n\nggplot(data = dat,\n       mapping = aes(x = group, y = y)) +\n  geom_boxplot() +\n  labs(x = \"\", y = \"Response\") +\n  facet_wrap(~ dataset)\n```\n\nNearly everyone would say that Dataset A provides stronger evidence of a relationship between the grouping variable and the response.  We generated these data such that the mean for Groups I, II and III are 5, 6 and 7, respectively, _for both Datasets A and B_.  While there is a difference, on average, in the response across the groups in both cases, it is correct that Dataset A provides stronger evidence for that relationship.  The real question is \"what is it that leads everyone to make the same conclusion when we have not yet discussed how to analyze this data?\"  When we ask students why they feel Dataset A provides stronger evidence, we typically hear that it is because the \"gaps\" between the groups \"look bigger.\"  Exactly!\n\n\n## Partitioning Variability\nSubconsciously, when we are deciding whether there is a difference in the average response between the groups, we are partitioning the variability in the response.  We are essentially describing two sources of variability: the variability in the response caused by subjects belonging to different groups and the variability in the response within a group (@fig-anovateststat-partition-variability).  In both Datasets A and B from @fig-anovateststat-boxplots, the __between-group variability__ is the same; the difference in the means from one group to another is the same for both datasets.  However, the __within-group variability__ is much smaller for Dataset A compared to Dataset B.  \n\n```{r}\n#| label: fig-anovateststat-partition-variability\n#| fig-cap: Illustration of partitioning the variability in the response to assess the strength of a signal.\n#| fig-alt: Illustration of three boxplots; the difference between the group means captures the between-group variability and the spread of the boxes captures the within-group variability.\nknitr::include_graphics(\"./images/ANOVATestStat-Partition-Variability.jpg\")\n```\n\n:::{#def-between-group-variability}\n## Between Group Variability\nWhen comparing a quantitative response across groups, the between group variability is the variability in the average response from one group to another.\n:::\n\n:::{#def-within-group-variability}\n## Within Group Variability\nWhen comparing a quantitative response across groups, the within group variability is the variability in the response within each group.\n:::\n\nThe power of @fig-anovateststat-boxplots is that it allows us to examine the between group variability (how the average responses differ from one another) _relative to_ the within group variability (how the responses within a group differ from one another).  What we see is that the larger this ratio, the stronger the signal.   Quantifying the strength of a signal is then about quantifying the ratio of these two sources of variability.  Let this sink in because it is completely counterintuitive.  We are saying that in order to determine if there is a difference in the mean response across groups, we have to examine variability.  Further, a signal in data is measured by the variability it produces.  It is for this reason that comparing a quantitative response across a categorical variable is termed ANalysis Of VAriance (ANOVA).\n\n:::{.callout-tip}\n## Big Idea\nConsider the ratio of the variability between groups to the variability within groups.  The larger this ratio, the stronger the evidence of a signal provided by the data.\n:::\n\nThis partitioning is a bit easier to visualize here than it was for the simple linear regression model, but the process is actually exactly the same.\n\n\n## Forming a Standardized Test Statistic\nLet's return to our model for the moral expectation score as a function of the food exposure group given in @eq-anovamodel-model:\n\n$$(\\text{Moral Expectations})_i = \\mu_1 (\\text{Comfort})_i + \\mu_2 (\\text{Control})_i + \\mu_3 (\\text{Organic})_i + \\varepsilon_i,$$\n\nwhere we use the same indicator variables defined in @sec-anovamodel.  We were interested in the following research question:\n\n  > Does the average moral expectation score differ for at least one of the three food exposure groups?\n\nThis was captured by the following hypotheses:\n\n  > $H_0: \\mu_1 = \\mu_2 = \\mu_3$  \n  > $H_1: \\text{at least one } \\mu_j \\text{ differs}.$\n  \nAs we stated above, quantifying the strength of a signal is equivalent to quantifying the ratio of two sources of variability.  This ratio will form our standardized statistic.  Our model acknowledges these two sources of variability; the question we then have before us is the following: how do we measure these sources of variability?\n\nAs with the linear regression model, we want to move forward with a goal of trying to say something like\n\n$$\\begin{pmatrix} \\text{Total Variability} \\\\ \\text{in the Moral Expectations} \\end{pmatrix} = \\begin{pmatrix} \\text{Variability due} \\\\ \\text{to Food Exposure} \\end{pmatrix} + \\begin{pmatrix} \\text{Variability due} \\\\ \\text{to Noise} \\end{pmatrix}$$\n\nAs we have seen in @sec-summaries, @sec-teststat, and in @sec-regquality, variability can be quantified through considering the \"total\" distance the observations are from a common target (for example, the mean response) where \"distance\" is captured by squared deviations.  That is, the total variability in the moral expectation score can be measured by\n\n$$\\sum_{i=1}^{n} \\left[(\\text{Moral Expectation})_i - (\\text{Overall Mean Moral Expectation})\\right]^2.$$ {#eq-anovateststat-sst}\n\nNotice this quantity is related to, but is not equivalent to, the sample variance.  It measures the distance each response is from the sample mean and then adds these distances up.  This __Total Sum of Squares__ is exactly the same as we developed for the regression model (@def-sst):\n\n:::{.theorem.definition}\n```{r}\n#| label: anovateststat-get-def-sst\n#| output: asis\n\nread_lines('03h-regquality.qmd') |>\n  repeat_block(pattern = ':::\\\\{#def-sst') |>\n  get_terms(term = 'fullblock') |>\n  cat()\n```\n:::\n\nWe now have a way of quantifying the total variability in the moral expectation scores we observed; we now want to partition (or separate) out this variability into its two components.  In order to capture the variability due to the food exposure groups, we consider how it plays a role in the model for the data generating process: it allows participants from different groups to have a different mean response.  That is, the deterministic portion of the model for the data generating process is the model's attempt to explain how changes in the group explain changes in the moral expectation score.  Finding the variability in the moral expectations due to the food exposure groups is then equivalent to finding the variability among these estimated (or predicted) mean responses:\n\n$$\\sum_{i=1}^{n} \\left[(\\text{Group Mean Moral Expectation})_i - (\\text{Overall Mean Moral Expectation})\\right]^2.$$ {#eq-anovateststat-ssr}\n\nThis term quantifies the variability explained by the groups, and it is called the __Treatment Sum of Squares__, but it is equivalent to the __Regression Sum of Squares__ (@def-ssr):\n\n:::{.theorem.definition}\n```{r}\n#| label: anovateststat-get-def-ssr\n#| output: asis\n\nread_lines('03h-regquality.qmd') |>\n  repeat_block(pattern = ':::\\\\{#def-ssr') |>\n  get_terms(term = 'fullblock') |>\n  str_c('\\n\\nThis is also known as the Treatment Sum of Squares (abbreviated SSTrt) in ANOVA.', collapse = '') |>\n  cat()\n```\n:::\n\nWe need to be careful here that the overall mean response is the sample mean across all groups, while the \"predicted mean response\" is the observed sample mean within each group.\n\nFinally, the unexplained noise, $\\varepsilon$ in our model for the data generating process, is the difference between the actual response and the deterministic portion of the model (in our case, the true mean response in each group).  This variability in the noise is then the variability within each group:\n\n$$\\sum_{i=1}^{n} \\left[(\\text{Moral Expectation})_i - (\\text{Group Mean Moral Expectation})_i\\right]^2.$$ {eq-anovateststat-sse}\n\nThis __Error Sum of Squares__ is exactly the same as we developed for the regression model (@def-sse):\n\n:::{.theorem.definition}\n```{r}\n#| label: anovateststat-get-def-sse\n#| output: asis\n\nread_lines('03h-regquality.qmd') |>\n  repeat_block(pattern = ':::\\\\{#def-sse') |>\n  get_terms(term = 'fullblock') |>\n  cat()\n```\n:::\n\nAgain, the \"predicted mean response\" for ANOVA is the observed sample mean within each group.\n\n:::{.callout-tip}\n## Big Idea\nThe total variability in a response can be partitioned into two components: the variability explained by the predictor and the unexplained variability left in the error term.  This is represented in the formula\n\n$$SST = SSR + SSE$$\n:::\n\nAs we have seen repeatedly, hypothesis testing is really model comparison; that is, our hypotheses of interest comparing the means really suggest two separate models for the data generating process:\n\n$$\n\\begin{aligned}\n  \\text{Model 1}:& \\quad (\\text{Moral Expectation})_i = \\mu_1 (\\text{Comfort})_i + \\mu_2 (\\text{Control})_i + \\mu_3 (\\text{Organic})_i + \\varepsilon_i \\\\\n  \\text{Model 0}:& \\quad (\\text{Moral Expectation})_i = \\mu + \\varepsilon_i,\n\\end{aligned}\n$$\n\nwhere $\\mu$ represents the common value of $\\mu_1, \\mu_2, \\mu_3$ under the null hypothesis (that is, the shared overall mean moral expectation score).  The model under the null hypothesis (Model 0) has fewer parameters because it is a constrained version of Model 1 resulting from setting $\\mu_1 = \\mu_2 = \\mu_3$ (the common value of which we called $\\mu$).  In fact, while Model 1 says there are two components (food exposure group and noise) contributing to the variability observed in the moral expectations, Model 0 says that there is only a single component (noise).  \n\nRegardless of which model we choose, the total variability in the response remains the same.  We are simply asking whether the variability explained by the food exposure group is sufficiently large for us to say it has an impact.  In particular, if the null hypothesis were true, we would expect all the variability in the response to be channeled into the noise ($SST \\approx SSE$).  In fact, think about computing the error sum of squares for Model 0 above; it would be\n\n$$SSE_0 = \\sum_{i=1}^{n} \\left[(\\text{Moral Expectations})_i - (\\text{Overall Mean Moral Expectation})\\right]^2$$\n\nsince the least squares estimate of $\\mu$ in Model 0 is the sample mean (see @sec-app-teststat).  But, this is equivalent to the total sum of squares for Model 1 (@eq-anovateststat-sst).  This confirms our intuition that if the null hypothesis were true, we would expect all the variability in the response to be channeled into the noise.\n\nIf, however, the alternative hypothesis is true and the food exposure group explains some portion of the variability in the moral expectations, then we would expect some of the variability to be channeled out of the noise term ($SSR > 0$).  Because we have partitioned the variability, we now take a moment to recognize that\n\n$$SSR = SST - SSE,$$\n\nbut we know that the total sum of squares is just the error sum of squares from the reduced model (Model 0) as shown above.  Therefore, we can write\n\n$$SSR = SSE_0 - SSE_1,$$ {#eq-anovateststat-ssr-difference}\n\nwhere we use the subscripts to denote whether we are discussing the error sum of squares from the reduced model (Model 0) or the full unconstrained model (Model 1).  That is, @eq-regquality-ssr-difference reveals that the regression sum of squares is the equivalent of the shift in the error sum of squares as we move from the reduced model under the null hypothesis to the more complex model under the alternative hypothesis.\n\n:::{.callout-tip}\n## Big Idea\nFor a particular dataset, the regression sum of squares quantifies the shift in the error sum of squares as we move from a reduced model to a more complex model.  It measures the \"signal\" in the data represented by the more complex model for the data generating process.\n:::\n\nJust as we stated in @sec-regquality, while sums of squares partition the variability, mean squares quantify the actual variance, and it turns out working with variances is beneficial.  To move from a sum of squares to a mean square, we need to divide by the __degrees of freedom__ (@def-df).\n\n:::{.theorem.definition}\n```{r}\n#| label: anovateststat-get-def-df\n#| output: asis\n\nread_lines('03h-regquality.qmd') |>\n  repeat_block(pattern = ':::\\\\{#def-df') |>\n  get_terms(term = 'fullblock') |>\n  cat()\n```\n:::\n\n:::{#def-ms-anova}\n## Mean Square (in ANOVA)\nA mean square is the ratio of a sum of squares and its corresponding degrees of freedom.  For a model of the form in @eq-anova, we have\n\n  - __Mean Square Total (MST)__: estimated variance of the responses; this is the same as the sample variance of the response.\n  - __Mean Square for Regression (MSR)__: estimated variance of the sample mean responses from each group; this is also called the Mean Square for Treatment (MSTrt) in ANOVA.\n  - __Mean Square Error (MSE)__: estimated variance of the error terms; this is equivalent to the estimated variance of the response within a group.\n  \nIn each case, the mean square is an estimated variance.  These are equivalent to the MST, MSR, and MSE in the regression model (@def-ms).\n:::\n\nThe MSR quantifies the between group variability and the MSE quantifies the within group variability.  The MSR is our signal.  The larger this variance, the further apart the observed group sample means are from one another (providing evidence for the alternative hypothesis); the smaller this variance, the closer the observed group sample means are (consistent with the null hypothesis).  \n\n:::{.callout-note}\n## Degrees of Freedom for MSR (or MSTrt)\nIn an ANOVA model, the MSR is capturing the variability among $k$ sample means; therefore, the associated degrees of freedom are $k - 1$.  \n:::\n\nMSE provides a measure of the noise within the data.  Again, in @fig-anovateststat-boxplots, the variability between the means is identical for the two datasets; the signal is easier to discern for Dataset A because this variability is larger _with respect to the noise_.  \n\n:::{.callout-note}\n## Applicability of MSE\nThe MSE is a pooled estimate of the variance within a group.  That is, it is a weighted average of the observed group sample variances.  Therefore, interpreting this value only makes sense if we are willing to impose the constant-variance condition.  However, regardless of whether we impose the constant-variance condition, the MSE is helpful in computing a standardized statistic.\n:::\n\n:::{.callout-note}\n## Degrees of Freedom for MSE\nIn an ANOVA model, predicting the mean response requires estimating $k$ parameters (the sample mean within each of the $k$ groups); therefore, the associated degrees of freedom are $n - k$.\n:::\n\nWe are now ready to define our standardized statistic as the ratio of mean squares:\n\n$$\\frac{MSR}{MSE} = \\frac{\\left(SST - SSE_1\\right)/(k - 1)}{SSE_1 / (n - k)} = \\frac{\\left(SSE_0 - SSE_1\\right)/(k - 1)}{SSE_1 / (n - k)},$$\n\nwhere again we have added subscripts to emphasize from which model we are computing the above sums of squares.  This standardized statistic could be used to quantify the signal-to-noise ratio (the amount of evidence) in the sample for testing the hypotheses\n\n  > $H_0: \\mu_1 = \\mu_2 = \\mu_3$  \n  > $H_1: \\text{At least one } \\mu_j \\text{ differs}$\n  \nHowever, we can generalize this for testing a range of hypotheses within our model for the data generating process.\n\n:::{#def-anova-f}\n## Standardized Statistic for ANOVA\nConsider testing a set of hypotheses for a model of the data generating process of the form (@eq-anova):\n\n$$(\\text{Response})_i = \\sum_{j=1}^{k} \\mu_j(\\text{Group } j)_i + \\varepsilon_i,$$\n\nwhere \n\n$$(\\text{Group } j)_i = \\begin{cases} 1 & \\text{i-th unit belongs to group } j \\\\ 0 & \\text{otherwise} \\end{cases}$$ \n\nis an indicator variable.  Denote this model as Model 1, and denote the model that results from applying the parameter constraints defined under the null hypothesis as Model 0.  A standardized statistic, sometimes called the \"standardized F statistic,\" for testing the hypotheses is given by\n\n$$T^* = \\frac{\\left(SSE_0 - SSE_1\\right) / (k - r)}{SSE_1 / (n - k)},$$\n\nwhere $k$ is the number of parameters in the full unconstrained model and $r$ is the number of parameters in the reduced model.  Defining \n\n$$MSA = \\frac{SSE_0 - SSE_1}{k - r}$$\n\nto be the \"mean square for additional terms,\" which captures the shift in the error sum of squares from the reduced model to the full unconstrained model, we can write the standardized statistic as\n\n$$T^* = \\frac{MSA}{MSE}$$\n\nwhere the mean square error in the denominator comes from the full unconstrained model.  Just as before, the MSE represents the residual variance --- the variance in the response for a particular set of the predictors.\n:::\n\nIt can be shown that this standardized statistic is a special case of the one discussed in @sec-regextensions and is therefore consistent with the ones presented in @sec-regquality and @sec-teststat.  The numerator captures the signal by examining the difference between what we expect the error sum of squares to be under the null hypothesis and what we actually observe; the denominator captures the background noise (relative to the estimated mean response from the full model).  Larger values of this standardized statistic indicate more evidence in the sample against the null hypothesis.\n\nWe should not lose sight of the fact that our standardized statistic is really a result of partitioning the variability and considering the variability explained by the factor of interest relative to the noise in the response.  Underscoring that the standardized statistic is a result of this partitioning, the analyses of these sources of variability is often summarized in a table similar to that represented in @fig-regquality-ANOVA-table, which is called an \"ANOVA table\" (@fig-anovateststat-anova-table).\n\n```{r}\n#| label: fig-anovateststat-anova-table\n#| fig-cap: Layout of an ANOVA table which summarizes the analysis conducted.  Emphasis is on partitioning the variability.\n#| fig-alt: A table with three rows; the first contains elements related to the variability explained by the factor of interest, and the second contains elements related to the variability not explained by the factor of interest.  It is shown how these components create the standardized statistic.\nknitr::include_graphics(\"./images/ANOVAteststat-Table.jpg\")\n```\n\nThis table is extremely familiar as we encountered it in @sec-regquality.  Just as before, the last entry in the table is the p-value.  As with any p-value, it is computed by finding the likelihood, assuming the null hypothesis is true, of getting, by chance alone, a standardized statistic as extreme or more so than that observed in our sample.  \"More extreme\" values of the statistic would be larger values; so, the area under the null distribution to the right of the observed statistic is the p-value.  \n\nWe note that while mathematical formulas have been provided to add some clarity to those who think algebraically, our emphasis is _not_ on the computational formulas as much as the idea that we are comparing two sources of variability.\n\nLet's return to the question that inspired our investigation in this chapter:\n\n  > Does the average moral expectation score differ for at least one of the three food exposure groups?\n\nThis was captured by the following hypotheses:\n\n  > $H_0: \\mu_1 = \\mu_2 = \\mu_3$  \n  > $H_1: \\text{at least one } \\mu_j \\text{ differs}.$\n\n@tbl-anovateststat-anova gives the ANOVA table summarizing the partitioned sources of variability in the moral expectation score.  We have a large p-value (computed assuming the data is consistent with the classical ANOVA model).  That is, the sample provides no evidence to suggest the average moral expectation differs across any of the food exposure groups.  The study suggests it is reasonable to assume that the foods we are exposed to do not impact our moral expectation, on average.\n\n```{r}\n#| label: tbl-anovateststat-anova\n#| tbl-cap: Analysis of the sources of variability in the moral expectation sore as a function of the food exposure groups.\n\nfit.organic <- lm(moral_avg ~ Food_Condition - 1, data = organic.df)\n\nfit.organic |>\n  compare_models(\n    reduced.mean.model = lm(moral_avg ~ 1, data = organic.df),\n    assume.constant.variance = TRUE,\n    assume.normality = TRUE\n  ) |>\n  mutate(p.value = ifelse(p.value>=0.001, round(p.value, 3), \"< 0.001\")) |>\n  select(Term = source,\n         DF = df,\n         `Sum of Squares` = ss,\n         `Mean Square` = ms,\n         `Standardized Statistic` = standardized.statistic,\n         `P-Value` = p.value) |>\n  mutate(Term = recode(Term, \n                       \"Additional Terms\" = \"Food Exposure Group\")) |>\n  mykable(digits = 3) |>\n  kableExtra::kable_styling(\n    latex_options = c('striped', 'scale_down')\n  )\n```\n\n\n:::{.callout-tip}\n## Big Idea\nDetermining if a response is related to a categorical predictor is done by determining if the predictor explains a significant portion of the variability in the response.\n:::\n\nIn this chapter, we partitioned variability as a way of evaluating the strength of evidence the predictor plays in determining the response.  As with the linear regression model, partitioning the variability is a key step.  By partitioning the variability in the response, we are able to construct a standardized statistic for testing the hypothesis of interest.  The model for the null distribution of this statistic depends upon the conditions we are willing to impose on the stochastic portion of the data generating process.  Regardless of the conditions we impose, we can interpret the resulting p-value similarly.  It provides an indication of whether the data suggests that the average response differs for at least one of the groups.\n\nOf course, the interpretation of the p-value depends on the conditions we impose.  We should not choose such conditions without performing some type of assessment to ensure those conditions are reasonable --- that the data is consistent with the conditions.  That is the focus of the next chapter.\n\n","srcMarkdownNoYaml":""},"formats":{"html":{"identifier":{"display-name":"HTML","target-format":"html","base-format":"html"},"execute":{"fig-width":7,"fig-height":5,"fig-format":"retina","fig-dpi":96,"df-print":"default","error":false,"eval":true,"cache":null,"freeze":false,"echo":false,"output":true,"warning":false,"include":true,"keep-md":false,"keep-ipynb":false,"ipynb":null,"enabled":null,"daemon":null,"daemon-restart":false,"debug":false,"ipynb-filters":[],"ipynb-shell-interactivity":null,"plotly-connected":true,"engine":"knitr"},"render":{"keep-tex":false,"keep-typ":false,"keep-source":false,"keep-hidden":false,"prefer-html":false,"output-divs":true,"output-ext":"html","fig-align":"center","fig-pos":null,"fig-env":null,"code-fold":"none","code-overflow":"scroll","code-link":false,"code-line-numbers":false,"code-tools":false,"tbl-colwidths":"auto","merge-includes":true,"inline-includes":false,"preserve-yaml":false,"latex-auto-mk":true,"latex-auto-install":true,"latex-clean":true,"latex-min-runs":1,"latex-max-runs":10,"latex-makeindex":"makeindex","latex-makeindex-opts":[],"latex-tlmgr-opts":[],"latex-input-paths":[],"latex-output-dir":null,"link-external-icon":false,"link-external-newwindow":false,"self-contained-math":false,"format-resources":[],"notebook-links":true},"pandoc":{"standalone":true,"wrap":"none","default-image-extension":"png","to":"html","css":["mystyles.css"],"output-file":"04h-anovateststat.html"},"language":{"toc-title-document":"Table of contents","toc-title-website":"On this page","related-formats-title":"Other Formats","related-notebooks-title":"Notebooks","source-notebooks-prefix":"Source","other-links-title":"Other Links","code-links-title":"Code Links","launch-dev-container-title":"Launch Dev Container","launch-binder-title":"Launch Binder","article-notebook-label":"Article Notebook","notebook-preview-download":"Download Notebook","notebook-preview-download-src":"Download Source","notebook-preview-back":"Back to Article","manuscript-meca-bundle":"MECA Bundle","section-title-abstract":"Abstract","section-title-appendices":"Appendices","section-title-footnotes":"Footnotes","section-title-references":"References","section-title-reuse":"Reuse","section-title-copyright":"Copyright","section-title-citation":"Citation","appendix-attribution-cite-as":"For attribution, please cite this work as:","appendix-attribution-bibtex":"BibTeX citation:","title-block-author-single":"Author","title-block-author-plural":"Authors","title-block-affiliation-single":"Affiliation","title-block-affiliation-plural":"Affiliations","title-block-published":"Published","title-block-modified":"Modified","title-block-keywords":"Keywords","callout-tip-title":"Tip","callout-note-title":"Note","callout-warning-title":"Warning","callout-important-title":"Important","callout-caution-title":"Caution","code-summary":"Code","code-tools-menu-caption":"Code","code-tools-show-all-code":"Show All Code","code-tools-hide-all-code":"Hide All Code","code-tools-view-source":"View Source","code-tools-source-code":"Source Code","tools-share":"Share","tools-download":"Download","code-line":"Line","code-lines":"Lines","copy-button-tooltip":"Copy to Clipboard","copy-button-tooltip-success":"Copied!","repo-action-links-edit":"Edit this page","repo-action-links-source":"View source","repo-action-links-issue":"Report an issue","back-to-top":"Back to top","search-no-results-text":"No results","search-matching-documents-text":"matching documents","search-copy-link-title":"Copy link to search","search-hide-matches-text":"Hide additional matches","search-more-match-text":"more match in this document","search-more-matches-text":"more matches in this document","search-clear-button-title":"Clear","search-text-placeholder":"","search-detached-cancel-button-title":"Cancel","search-submit-button-title":"Submit","search-label":"Search","toggle-section":"Toggle section","toggle-sidebar":"Toggle sidebar navigation","toggle-dark-mode":"Toggle dark mode","toggle-reader-mode":"Toggle reader mode","toggle-navigation":"Toggle navigation","crossref-fig-title":"Figure","crossref-tbl-title":"Table","crossref-lst-title":"Listing","crossref-thm-title":"Theorem","crossref-lem-title":"Lemma","crossref-cor-title":"Corollary","crossref-prp-title":"Proposition","crossref-cnj-title":"Conjecture","crossref-def-title":"Definition","crossref-exm-title":"Example","crossref-exr-title":"Exercise","crossref-ch-prefix":"Chapter","crossref-apx-prefix":"Appendix","crossref-sec-prefix":"Section","crossref-eq-prefix":"Equation","crossref-lof-title":"List of Figures","crossref-lot-title":"List of Tables","crossref-lol-title":"List of Listings","environment-proof-title":"Proof","environment-remark-title":"Remark","environment-solution-title":"Solution","listing-page-order-by":"Order By","listing-page-order-by-default":"Default","listing-page-order-by-date-asc":"Oldest","listing-page-order-by-date-desc":"Newest","listing-page-order-by-number-desc":"High to Low","listing-page-order-by-number-asc":"Low to High","listing-page-field-date":"Date","listing-page-field-title":"Title","listing-page-field-description":"Description","listing-page-field-author":"Author","listing-page-field-filename":"File Name","listing-page-field-filemodified":"Modified","listing-page-field-subtitle":"Subtitle","listing-page-field-readingtime":"Reading Time","listing-page-field-wordcount":"Word Count","listing-page-field-categories":"Categories","listing-page-minutes-compact":"{0} min","listing-page-category-all":"All","listing-page-no-matches":"No matching items","listing-page-words":"{0} words"},"metadata":{"lang":"en","fig-responsive":true,"quarto-version":"1.4.555","bibliography":["refs223notes.bib","packages.bib"],"comments":{"hypothesis":false},"fig-cap-location":"bottom","theme":"cosmo"},"extensions":{"book":{"multiFile":true}}},"pdf":{"identifier":{"display-name":"PDF","target-format":"pdf","base-format":"pdf"},"execute":{"fig-width":5.5,"fig-height":3.5,"fig-format":"pdf","fig-dpi":300,"df-print":"default","error":false,"eval":true,"cache":null,"freeze":false,"echo":false,"output":true,"warning":false,"include":true,"keep-md":false,"keep-ipynb":false,"ipynb":null,"enabled":null,"daemon":null,"daemon-restart":false,"debug":false,"ipynb-filters":[],"ipynb-shell-interactivity":null,"plotly-connected":true,"engine":"knitr"},"render":{"keep-tex":true,"keep-typ":false,"keep-source":false,"keep-hidden":false,"prefer-html":false,"output-divs":true,"output-ext":"pdf","fig-align":"center","fig-pos":null,"fig-env":null,"code-fold":"none","code-overflow":"scroll","code-link":false,"code-line-numbers":false,"code-tools":false,"tbl-colwidths":true,"merge-includes":true,"inline-includes":false,"preserve-yaml":false,"latex-auto-mk":true,"latex-auto-install":true,"latex-clean":true,"latex-min-runs":1,"latex-max-runs":10,"latex-makeindex":"makeindex","latex-makeindex-opts":[],"latex-tlmgr-opts":[],"latex-input-paths":[],"latex-output-dir":null,"link-external-icon":false,"link-external-newwindow":false,"self-contained-math":false,"format-resources":[]},"pandoc":{"pdf-engine":"xelatex","standalone":true,"variables":{"graphics":true,"tables":true},"default-image-extension":"pdf","to":"pdf","output-file":"04h-anovateststat.pdf"},"language":{"toc-title-document":"Table of contents","toc-title-website":"On this page","related-formats-title":"Other Formats","related-notebooks-title":"Notebooks","source-notebooks-prefix":"Source","other-links-title":"Other Links","code-links-title":"Code Links","launch-dev-container-title":"Launch Dev Container","launch-binder-title":"Launch Binder","article-notebook-label":"Article Notebook","notebook-preview-download":"Download Notebook","notebook-preview-download-src":"Download Source","notebook-preview-back":"Back to Article","manuscript-meca-bundle":"MECA Bundle","section-title-abstract":"Abstract","section-title-appendices":"Appendices","section-title-footnotes":"Footnotes","section-title-references":"References","section-title-reuse":"Reuse","section-title-copyright":"Copyright","section-title-citation":"Citation","appendix-attribution-cite-as":"For attribution, please cite this work as:","appendix-attribution-bibtex":"BibTeX citation:","title-block-author-single":"Author","title-block-author-plural":"Authors","title-block-affiliation-single":"Affiliation","title-block-affiliation-plural":"Affiliations","title-block-published":"Published","title-block-modified":"Modified","title-block-keywords":"Keywords","callout-tip-title":"Tip","callout-note-title":"Note","callout-warning-title":"Warning","callout-important-title":"Important","callout-caution-title":"Caution","code-summary":"Code","code-tools-menu-caption":"Code","code-tools-show-all-code":"Show All Code","code-tools-hide-all-code":"Hide All Code","code-tools-view-source":"View Source","code-tools-source-code":"Source Code","tools-share":"Share","tools-download":"Download","code-line":"Line","code-lines":"Lines","copy-button-tooltip":"Copy to Clipboard","copy-button-tooltip-success":"Copied!","repo-action-links-edit":"Edit this page","repo-action-links-source":"View source","repo-action-links-issue":"Report an issue","back-to-top":"Back to top","search-no-results-text":"No results","search-matching-documents-text":"matching documents","search-copy-link-title":"Copy link to search","search-hide-matches-text":"Hide additional matches","search-more-match-text":"more match in this document","search-more-matches-text":"more matches in this document","search-clear-button-title":"Clear","search-text-placeholder":"","search-detached-cancel-button-title":"Cancel","search-submit-button-title":"Submit","search-label":"Search","toggle-section":"Toggle section","toggle-sidebar":"Toggle sidebar navigation","toggle-dark-mode":"Toggle dark mode","toggle-reader-mode":"Toggle reader mode","toggle-navigation":"Toggle navigation","crossref-fig-title":"Figure","crossref-tbl-title":"Table","crossref-lst-title":"Listing","crossref-thm-title":"Theorem","crossref-lem-title":"Lemma","crossref-cor-title":"Corollary","crossref-prp-title":"Proposition","crossref-cnj-title":"Conjecture","crossref-def-title":"Definition","crossref-exm-title":"Example","crossref-exr-title":"Exercise","crossref-ch-prefix":"Chapter","crossref-apx-prefix":"Appendix","crossref-sec-prefix":"Section","crossref-eq-prefix":"Equation","crossref-lof-title":"List of Figures","crossref-lot-title":"List of Tables","crossref-lol-title":"List of Listings","environment-proof-title":"Proof","environment-remark-title":"Remark","environment-solution-title":"Solution","listing-page-order-by":"Order By","listing-page-order-by-default":"Default","listing-page-order-by-date-asc":"Oldest","listing-page-order-by-date-desc":"Newest","listing-page-order-by-number-desc":"High to Low","listing-page-order-by-number-asc":"Low to High","listing-page-field-date":"Date","listing-page-field-title":"Title","listing-page-field-description":"Description","listing-page-field-author":"Author","listing-page-field-filename":"File Name","listing-page-field-filemodified":"Modified","listing-page-field-subtitle":"Subtitle","listing-page-field-readingtime":"Reading Time","listing-page-field-wordcount":"Word Count","listing-page-field-categories":"Categories","listing-page-minutes-compact":"{0} min","listing-page-category-all":"All","listing-page-no-matches":"No matching items","listing-page-words":"{0} words"},"metadata":{"block-headings":true,"bibliography":["refs223notes.bib","packages.bib"],"comments":{"hypothesis":false},"fig-cap-location":"bottom","documentclass":"scrreprt"},"extensions":{"book":{"selfContainedOutput":true}}}},"projectFormats":["html","pdf"]}